{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization for Movie Recommender System using Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source**: \"ratings.csv\", download from Kaggle at \"https://www.kaggle.com/rounakbanik/the-movies-dataset/version/7#ratings.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ratings.csv has the columns userId, movieId, rating, and timestamp. Our goal is to produce latent vectors for every movie and every user through stochastic gradient descent (SGD).\n",
    "\n",
    "We will use the userId, movieId, and rating fields to produce a ratings matrix $R$, where each row represents a user and each column represents a movie, and hence each matrix entry is the rating some user gives to some movie. Note that $R$ be sparse i.e. there will be missing entries. These are the entries that we are predicting.   \n",
    "\n",
    "We use SGD is to estimate the principal component analysis (SVD) matrix factorization of our ratings matrix (with a preselected dimension $k$), such that we can produce latent vectors for both users and movies. In this way, we can predict the rating for any user-movie combination (by simply taking a dot product!). $k$ represents the dimension of the latent vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Review of  SVD for our Movie Recommendation Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD decomposes any matrix into into  $U \\Sigma V^T$. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors. $\\Sigma$ is a square diagnoal matrix with the eigenvalues of $R$ along the diagonal. $\\Sigma$ can be multiplied into either $U$ or $V^T$. We choose $U$ for our example. Let $U^\\prime$ equal $U \\Sigma$. Recall that $R$ is our ratings matrix. Therefore, we have that:\n",
    "\n",
    "$R = U\\Sigma V^T = U^\\prime V^T$\n",
    "\n",
    "In this case, the rows of $U^\\prime$ represent the latent user vectors, and the rows of $V$ represent the latent movie vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "The functions *generate_matrix* and *split_matrix* preprocess our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, random, time, scipy, operator\n",
    "from collections import OrderedDict\n",
    "from scipy import spatial, stats\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fname = 'ratings.csv'\n",
    "\n",
    "data = np.genfromtxt(csv_fname, delimiter=',', names=True, case_sensitive=True)\n",
    "\n",
    "movie_ids = []\n",
    "user_ids = []\n",
    "\n",
    "movie_ids_set = set()\n",
    "user_ids_set = set()\n",
    "\n",
    "int_movie_id_map = {}\n",
    "int_user_id_map = {}\n",
    "\n",
    "\"\"\"records all the coordinates of the matrix for which data has been provided\"\"\"\n",
    "data_coordinates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *generate_matrix*, we produce a matrix from the csv file. Each row in the matrix represents a user and each column represents a movie. Thus the entry $i,j$ of the matrix represents the rating that user $i$ gave to movie $j$. We also record coordinates of entries in $R$ provided in the data in the data_coordinates list (to differentiate from missing data entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate  Matrix from csv data\"\"\"\n",
    "def generate_matrix():\n",
    "    for element in data[\"movieId\"]:\n",
    "        if int(element) not in movie_ids_set:\n",
    "            movie_ids.append(int(element))\n",
    "            movie_ids_set.add(int(element))\n",
    "\n",
    "\n",
    "    for element in data[\"userId\"]:\n",
    "        if int(element) not in user_ids_set:\n",
    "            user_ids.append(int(element))\n",
    "            user_ids_set.add(int(element))\n",
    "\n",
    "\n",
    "    movie_ids.sort()\n",
    "    user_ids.sort()\n",
    "\n",
    "    matrix = np.ndarray(shape=(len(user_ids),len(movie_ids)), dtype= float)\n",
    "\n",
    "    for i in range(len(movie_ids)):\n",
    "        int_movie_id_map[movie_ids[i]] = i\n",
    "\n",
    "    for i in range(len(user_ids)):\n",
    "        int_user_id_map[user_ids[i]] = i  \n",
    "\n",
    "\n",
    "    for element in data:\n",
    "        x = int_user_id_map[element[\"userId\"]]\n",
    "        y = int_movie_id_map[element[\"movieId\"]]\n",
    "        matrix[x][y] = element[\"rating\"]\n",
    "        data_coordinates.append((x,y))\n",
    "        \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *split_matrix*, we shuffle the coordinates of the data in the *data_coordinates* list and make the split at a predetermined cut. e.g. with an $80/20$ train/test split, take the first $80%$ entries of data coordinates and use those as training data. Use the last $20%$ for testing.\n",
    "\n",
    "*split_matrix* returns a matrix with testing coordinates removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Produces a new matrix with 20% of the non-zero entries replaced by zeros\"\"\"\n",
    "def split_matrix(original_matrix,split = 0.8):\n",
    "    training_matrix = np.zeros((original_matrix.shape[0],original_matrix.shape[1]), dtype= float)\n",
    "\n",
    "    shuffled_data_coordinates = data_coordinates.copy()\n",
    "    random.shuffle(shuffled_data_coordinates)\n",
    "\n",
    "    eighty_cut = int(split*len(shuffled_data_coordinates))\n",
    "\n",
    "    training_coordinates = []\n",
    "    \n",
    "    testing_coordinates = []\n",
    "\n",
    "    for i in range(len(shuffled_data_coordinates)):\n",
    "        if i < eighty_cut:\n",
    "            training_coordinates.append(shuffled_data_coordinates[i])\n",
    "        else:\n",
    "            testing_coordinates.append(shuffled_data_coordinates[i])\n",
    " \n",
    "    for element in training_coordinates:\n",
    "        training_matrix[element[0]][element[1]] = original_matrix[element[0]][element[1]]\n",
    "    \n",
    "\n",
    "    return training_coordinates,testing_coordinates, training_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *run_model* we split our data into train and test, produce our latent vetors by calling *SGD*, and then compute the mean absolute error (MAE) and the mean square error (MSE) by calling *compute_error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(original_matrix, split):\n",
    "\n",
    "    training_coordinates, testing_coordinates, training_matrix = split_matrix(original_matrix, split)\n",
    "    \n",
    "    #u and v contain the latent vectors\n",
    "    u,v = SGD(training_matrix, training_coordinates)\n",
    "    error = compute_error(testing_coordinates, u, v, original_matrix)\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"use stochastic gradient descent to estimate the latent vectors for all users and all \n",
    "movies, these latent vectors can then predict any movie rating by taking the dot product\n",
    "between any user and any movie\n",
    "\"\"\"\n",
    "def SGD(training_matrix, training_coordinates):\n",
    "    '''Learn the vectors p_u and q_i with SGD.'''\n",
    "\n",
    "    n_factors = 15  # number of factors\n",
    "    alpha = .01  # learning rate\n",
    "    steps = 10  # number of iteration of the SGD procedure\n",
    "    \n",
    "    #randomly initialize the matrices U and V (represented by variables u and v)\n",
    "    u = np.random.normal(0, .1, (len(training_matrix), n_factors))\n",
    "    v = np.random.normal(0, .1, (len(training_matrix.T), n_factors))\n",
    "\n",
    "    # Optimization procedure\n",
    "    for x in range(steps):\n",
    "        for coordinates in training_coordinates:\n",
    "            i = coordinates[0]\n",
    "            j = coordinates[1]\n",
    "            \n",
    "            r_ij = training_matrix[i][j]\n",
    "            \n",
    "            err = r_ij - np.dot(u[i], v[j])\n",
    "            \n",
    "            #descend in direction of gradient as per the derivatives of loss \n",
    "            #function of r_ij\n",
    "            u[i] += alpha * err * v[j] \n",
    "            v[j] += alpha * err * u[i]\n",
    "            \n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(testing_coordinates, u, v, original_matrix):\n",
    "    \"\"\"Computes MAE and MSE for Matrix Factorization\"\"\"\n",
    "    \n",
    "    num_test_points = float(len(testing_coordinates))\n",
    "    \n",
    "    MSE = 0\n",
    "    MAE = 0\n",
    "    \n",
    "    for coordinate in testing_coordinates:\n",
    "        i = coordinate[0]\n",
    "        j = coordinate[1]\n",
    "        \n",
    "        true_value = original_matrix[i][j]\n",
    "        \n",
    "        predicted_value = np.dot(u[i], v[j])\n",
    "        \n",
    "        if predicted_value > 5:\n",
    "            predicted_value = 5\n",
    "        elif predicted_value < 0.5:\n",
    "            predicted_value = 0.5\n",
    "        \n",
    "        difference = true_value - predicted_value\n",
    "        \n",
    "        MSE += difference**2\n",
    "        MAE += abs(difference)\n",
    "        \n",
    "    MAE = MAE/num_test_points\n",
    "    MSE = MSE/num_test_points\n",
    "    \n",
    "    return MSE, MAE\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the codeblocks below to run the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_matrix is generated from the data in the cvs file\n",
    "original_matrix = generate_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2131072101861802, 0.8061933426009159)\n"
     ]
    }
   ],
   "source": [
    "run_model(original_matrix, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuples below represent (Mean Square Error(MSE), Mean Absolute Error(MAE)) over different training and testing splits. \n",
    "\n",
    "The train/test splits are as follows:\n",
    "\n",
    "\n",
    "$20/80, 30/70, 40/60 ... 80/20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0437326483111686, 1.3742378719956299)\n",
      "(2.075168870324216, 1.0885789859443213)\n",
      "(1.706594911459939, 0.9708634466677245)\n",
      "(1.4655814092655013, 0.8933423067277048)\n",
      "(1.352083452662624, 0.8561589637677941)\n",
      "(1.2573454121840755, 0.8256092741270032)\n",
      "(1.2126220751377417, 0.8081848463447346)\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    split = k*0.1\n",
    "    run_model(original_matrix, split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, it makes sense that the lowest error occurs when we have a larger amount of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
